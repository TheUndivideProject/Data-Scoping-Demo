---
title: "Filer Status Comparison"
output:
  quarto::html_document:
    toc: true

format:
  html:
    code-fold: true
---

## 1. Introduction

This document focuses on the distinctions between the three filing statuses outlined in the 2024 Q1 Submission Data Report – Large Accelerated Filers, Regular Accelerated Filers, and Non-Accelerated Filers – to provide a comprehensive view of how different market segments allocate their resources.

## 2. Research Questions

### 2.1. Selection Criteria

------------------------------------------------------------------------

Objective: Establish clear criteria for selecting companies within each filing status to ensure a balanced comparison.

1.  **Geographic Location**

    -   U.S. Based

    -   Presence of Environmental Justice Communities: Identified through Humanitarian OpenStreetMap Team (HOTOSM) field data. You can access the data on HOTOSM's GitHub: <https://github.com/hotosm/osm-fieldwork>

    -   Engagement with Digital Divide and Climate Change: Cities with a high presence of individuals and local agencies addressing the intersection of climate change and the digital divide, as indicated by responses to an international survey of emergency managers, disasterologists, planners, resilience professionals, academics, and NGOs.

        ```{python}
        # libraries for data manipulation.
        import pandas as pd
        import numpy as np

        # libraries for data visualisation
        import matplotlib.pyplot as plt
        import seaborn as sns
        import plotly.express as px
        import plotly.io as pio

        # libraries for quarto rendering
        from IPython.display import Markdown,display
        from tabulate import tabulate
        import plotly.io as pio

        # remove warnings
        import warnings
        warnings.filterwarnings("ignore", category=UserWarning)

        # read in data from 'emergency-manager-survey.csv' in the 'data' folder
        emergency_manager_survey = pd.read_csv('../data/emergency-manager-survey.csv')

        # print Q1-Q7
        display(Markdown("Questions:\n\n" +
                         emergency_manager_survey[['Q1', 'Q2', 'Q3', 'Q4', 'Q5', 'Q6', 'Q7']].head(1).to_markdown(index=False)))

        # print data dimensions
        shape_caption = "Data Dimensions:"
        shape_df = pd.DataFrame({
                'Dimension': ['Rows', 'Columns'],
                'Count': [emergency_manager_survey.shape[0], emergency_manager_survey.shape[1]]
            })
        shape_df['Count'] = shape_df['Count'].apply(lambda x: f"{x:,}")
        shape_markdown = shape_caption + "\n\n" + shape_df.to_markdown(index=False)
        display(Markdown(shape_markdown))
        ```

    -   Based on the combined criteria of environmental justice presence and engagement in addressing the digital divide and climate change, the following cities have been identified, listed in alphabetical order:

    -   ::: panel-tabset
        ## Target Cities

        -   Alexandria, VA
        -   Chicago, IL
        -   Fort Lauderdale, FL
        -   Laurel, MD
        -   Lewiston, ID
        -   Nashua, NH
        -   New Orleans, LA
        -   Portland, OR
        -   Raleigh, NC
        -   Rochester, NY
        -   Savannah, GA
        -   Seattle, WA
        -   Washington, DC
        -   Wichita, KS

        ## Target Cities Mapped

        ![](/data/target-cities.png)
        :::

    -   Number of Filers per Geographic Region as defined in Exempt Organizations Business Master File Extract (EO BMF) <https://www.irs.gov/charities-non-profits/exempt-organizations-business-master-file-extract-eo-bmf>:

        ```{python}
        # read in data from 'sub_data.csv' and 'num_data.csv' in the 'data' folder
        sub_data = pd.read_csv('../data/sub.csv')
        num_data = pd.read_csv('../data/num.csv')
        merged_df = pd.merge(sub_data, num_data, on='ACCESSION NUMBER')

        # count number of filers per geographic region
        regions = {
            'Northeast': ['CT', 'ME', 'MA', 'NH', 'NJ', 'NY', 'RI', 'VT'],
            'Mid-Atlantic and Great Lakes': ['DE', 'DC', 'IL', 'IN', 'IA', 'KY', 'MD', 'MI', 'MN', 'NE', 'NC', 'ND', 'OH', 'PA', 'SC', 'SD', 'VA', 'WV', 'WI'],
            'Gulf Coast and Pacific Coast': ['AL', 'AK', 'AR', 'AZ', 'CA', 'CO', 'FL', 'GA', 'HI', 'ID', 'KS', 'LA', 'MS', 'MO', 'MT', 'NV', 'NM', 'OK', 'OR', 'TX', 'TN', 'UT', 'WA', 'WY'],
            'Other': [] # international and all others
        }

        def find_region(state):
            for region, states in regions.items():
                if state in states:
                    return region
            return 'Other'

        merged_df['Region'] = merged_df['BA STATE'].apply(find_region)
        region_counts = merged_df['Region'].value_counts().reset_index()
        region_counts.columns = ['Region', 'Count']

        display(region_counts)
        ```

    -   Number of Filers per Geographic Region per Filer Status:

        ```{python}
        grouped = merged_df.groupby('FILER STATUS')
        region_counts_per_filer = pd.DataFrame()

        def add_counts_per_filer(grouped, field, field_name):
            counts_per_filer = []
            for name, group in grouped:
                counts = group[field].value_counts().reset_index()
                counts.columns = [field_name, 'Count']
                counts['FILER STATUS'] = name
                counts_per_filer.append(counts)
            return pd.concat(counts_per_filer)

        # count number of filers per geographic region per filer status
        region_counts_per_filer = add_counts_per_filer(grouped, 'Region', 'Region')

        # show the resulting dataframe
        display(region_counts_per_filer)
        ```

2.  **Industry**

    -   Some industries will be more inclined to donate to environmental causes than others.

    -   To reduce bias within each filer status, we can compare businesses within the same industries across different filer statuses.

    -   You can translate Standard Industrial Classification (SIC) Codes into industries using this [SIC Code List](https://www.sec.gov/corpfin/division-of-corporation-finance-standard-industrial-classification-sic-code-list).

    -   \# count number of filers per standard industrial classification

        classifications = sub_data\['CLASSIFICATION'\].value_counts().reset_index()

        classifications.columns = \['CLASSIFICATION', 'Count'\]

        display(classifications)

    -   Number of Filers per Standard Industrial Classification:

        ```{python}
        # count number of filers per standard industrial classification
        classifications = sub_data['CLASSIFICATION'].value_counts().reset_index()
        classifications.columns = ['CLASSIFICATION', 'Count']

        display(classifications)
        ```

    -   Number of Filers per Standard Industrial Classification per Filer Status:

        ```{python}
        classification_counts_per_filer = pd.DataFrame()

        # count number of filers per standard industrial classification per filer status
        classification_counts_per_filer = add_counts_per_filer(grouped, 'CLASSIFICATION', 'CLASSIFICATION')
        display(classification_counts_per_filer)
        ```

    -   The following ten industries appear most often in the Submissions Data Set and should thus be prioritized in our research to ensure a representative sample.

        ```{python}
        industries = [
            "Pharmaceutical Preparations",
            "Real Estate Investment Trusts",
            "Prepackaged Software",
            "State Commercial Banks",
            "Biological Products, Except Diagnostic Substances",
            "Fire, Marine, and Casualty Insurance",
            "Metal Cans",
            "Laboratory Analytical Instruments",
            "Calculating and Accounting Machines, Except Electronic Computers",
            "Miscellaneous Electrical Machinery, Equipment, and Supplies"
        ]

        classifications['INDUSTRY'] = pd.Series(industries)


        # show the resulting dataframe
        display(classifications.head(10))
        ```

3.  **Financial Performance**

    -   The Submissions Data Set identifies public float as a criterion for filer status, but that value varies quite a bit within each classification.

    -   Number of Filers per Public Float Percentile (considering entire Submissions Data Set):

        ```{python}
        # calculate public float quantiles
        def calculate_quantiles(df, element, quantiles=[0.25, 0.75]):
            ranges = []
            for name, group in grouped:
                values = group[group['FINANCIAL ELEMENT'] == element]['VALUE']
                quantile_values = values.quantile(quantiles)
                ranges.append({
                    'FILER STATUS': name,
                    'Bottom 25%': round(quantile_values[0.25]),
                    'Top 25%': round(quantile_values[0.75])
                })

            return pd.DataFrame(ranges)

        # merge and calculate public float percentiles
        public_floats = merged_df[merged_df['FINANCIAL ELEMENT'] == 'EntityPublicFloat'][['ACCESSION NUMBER', 'VALUE']]
        merged_df = merged_df.merge(public_floats, on='ACCESSION NUMBER', suffixes=('', '_public_float')).rename(columns={'VALUE_public_float': 'Public Float'})

        percentiles = merged_df['Public Float'].quantile([0.25, 0.75]).values
        merged_df['Public Float Percentile'] = pd.cut(merged_df['Public Float'], bins=[-float('inf'), percentiles[0], percentiles[1], float('inf')], labels=['Bottom 25%', 'Middle 50%', 'Top 25%'])

        public_float_counts = merged_df['Public Float Percentile'].value_counts().reset_index()
        public_float_counts.columns = ['Public Float Percentile', 'Count']

        display(public_float_counts)

        ```

    -   Number of Filers per Public Float Percentile per Filer Status (considering entire Submissions Data Set):

        ```{python}
        grouped = merged_df.groupby('FILER STATUS')
        public_float_counts_per_filer = pd.DataFrame()

        # count number of filers per standard industrial classification per filer status
        public_float_counts_per_filer = add_counts_per_filer(grouped, 'Public Float Percentile', 'Public Float Percentile')

        display(public_float_counts_per_filer)
        ```

    -   To ensure a balanced comparison, our assessment should target filers in the bottom 25%, middle 50%, and top 25% of each classification.

    -   Public Float Percentile Values within each Filer Status:

        ```{python}
        ranges_df = calculate_quantiles(merged_df, 'EntityPublicFloat')
        display(ranges_df)
        ```

### 2.2. Sample Size

------------------------------------------------------------------------

Objective: Define the optimal number of companies per classification for comparison.

Considering the data size, a sample size of three companies per classification is a good starting point. We can adjust this size if we encounter significant variations within a classification.

### 2.3. Data Points

------------------------------------------------------------------------

Objective: Select data points for comparison. This could include R&D expenditures, marketing expenses, compliance costs, and other relevant financial metrics.

-   The following data points (outlined in the Q1 Numeric Data Report) facilitate the assessment of an organization's expenses.

    -   **Accounting Fees:** provides insights into the organization's financial oversight and auditing practices. High fees can be associated with rigorous procedures.

    -   **Compensation:** provide insights into the organization's budget allocation for leadership compensation.

    -   **Interest:** provides insights into the organization's debt levels, which can inform assessments of financial health.

    -   **Legal Fees:** provides insights into the organization's legal compliance and potential litigation challenges. High fees can be associated with either.

-   In addition, an organization's spending on **Research & Development** can provide insights into its current priorities.

    -   **Research & Development Stats:**

        ```{python}
        import re

        expense_ids = ['ResearchAndDevelopmentExpense']
        expense_stats = pd.DataFrame()

        # add statistics for each identifier to combined DataFrame
        for id in expense_ids:
            filtered_expense_ids = num_data[num_data['FINANCIAL ELEMENT'] == id]
            stats = round(filtered_expense_ids['VALUE']).describe().rename(re.sub(r'(?<=[a-z])(?=[A-Z])', ' ', id))
            expense_stats = pd.concat([expense_stats, stats], axis=1)

        # show statistics
        display(expense_stats)
        ```

### 2.4. Analysis Framework

------------------------------------------------------------------------

Objective: Develop a framework for analyzing and comparing the resource allocation patterns.

1.  **Create Sample**

    -   **Criteria:** Reference [2.1. Selection Criteria].

    -   **Sample Size:** Three companies per classification (Large Accelerated Filers, Accelerated Filers, Non-Accelerated Filers).

2.  **Create Financial Profiles**

    -   **Use:** 2024 Q1 SEC Submissions and Numbers. Run the following code to create a merged dataset:

        ```{python}
        import pandas as pd

        # load the datasets
        num_data = pd.read_csv('../data/num.csv')
        sub_data = pd.read_csv('../data/sub.csv')

        # merge the datasets on 'ACCESSION NUMBER'
        merged_df = pd.merge(num_data, sub_data, on='ACCESSION NUMBER')

        # save the merged dataset to a CSV file
        # merged_df.to_csv('merged_dataset.csv', index=False)
        ```

    -   **Data Points:** Extract the following data points for each filer. Label missing data points 'nan.'

        ```{python}
        data_points = {
            'DATA POINT': [
                'Accounting Fees', 
                'Compensation', 
                '', 
                '',
                'Interest', 
                'Legal Fees', 
                'R&D Expenses'
            ],
            'KEYWORD WITHIN SEC DATA SET': [
                'AccountingFees', 
                'DeferredCompensationLiabilityCurrent', 
                'DeferredCompensationEquity', 
                'DeferredCompensationPlanAssets', 
                'InterestExpense', 
                'LegalFees', 
                'ResearchAndDevelopmentExpense'
            ]
        }

        data_points_df = pd.DataFrame(data_points)

        display(data_points_df)
        ```

3.  **Assess Charitable Giving**

    -   **Keywords:** “charitable activities”, “charitable contributions”, “charitable donations”, “community engagement”, “community investments”, “corporate social responsibility”, “philanthropy”

    -   **Method:**

        -   Search each filer for the above keywords within their 2023 Form 10-K and Form DEF 14-A using the [EDGAR Full Text Search](https://www.sec.gov/edgar/search/#).

        -   Extract (suggested data points):

            -   Quantitative Data:

                -   Amount donated to charitable causes.

                -   Number of charitable initiatives supported.

                -   Percentage of revenue donated to charitable causes.

            -   Qualitative Data:

                -   Descriptions of charitable initiatives or programs.

                -   Impact of charitable activities on communities.

4.  **Assess Climate Reporting**

    -   **Keywords:** "carbon disclosure", “climate action', "climate disclosure", "environmental compliance", "environmental governance", "environmental initiatives", "environmental policies", "ESG", "sustainable development goals", "SDG"

    -   **Method:**

        -   Search each filer for the above keywords within their 2023 Form 10-K, Form DEF 14A, and Form 8-K using the [EDGAR Full Text Search](https://www.sec.gov/edgar/search/#).

        -   Search each filer's 2023 Environmental Social Governance Report, if available online.

        -   Extract (suggested data points):

            -   Quantitative Data:

                -   Amount paid in environmental fines or penalties.

                -   Amount invested in environmental initiatives.

            -   Qualitative Data:

                -   Descriptions of climate-related risks and their potential impact on business operations.

                -   Governance structure for managing ESG issues.

                -   Stakeholder engagement on ESG issues.

                -   Descriptions of specific environmental projects or programs.

                -   Qualitative assessment of the impact of these initiatives.

5.  Data Collection and Documentation

    -   Use an Excel Spreadsheet to collect and document data.

    -   **Sample Spreadsheet Formatting:**

        ```{python}
        filer_comparison_sample_data = {
            'Name': [''],
            'Filing Status': [''],
            'Industry': [''],
            'Accounting Fees': [''],
            'Compensation': [''],
            'Interest': [''],
            'Legal Fees': [''],
            'R&D Expenses': [''],
            'Charitable Giving (10-K)': [''],
            'Charitable Giving (8-K)': [''],
            'Climate Reporting (10-K)': [''],
            'Climate Reporting (DEF 14A)': [''],
            'Climate Reporting (8-K)': [''],
            'Climate Reporting (ESG Report)': ['']
        }

        filer_comparison_sample_df = pd.DataFrame(filer_comparison_sample_data)
        display(filer_comparison_sample_df)
        ```
